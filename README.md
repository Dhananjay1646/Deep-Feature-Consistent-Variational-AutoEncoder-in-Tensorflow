# Deep Feature Consistent Variational Autoencoder in Tensorflow

This repository has an objective to implement Deep Feature Consisten Variational Autoencoder (DFC-VAE) according to ___paper____.
Tensorflow and Python3 are used for development, and pre-trained VGG16 is adapted from this __link___. 
Before reading this following note, it is recommended to understand the concept of Variational Autoencoder and generative model.

## Problem Statement

It is known that one major problem of plain Variational Autoencoder (Plain-VAE) is that images generated by the model are blurry. 
This is because the plain model's loss function is defined by pixel-wise comparison between input images and generated images. 
As a consequence, optimizing model to achieve a great performance is difficult since slightly shifting or distorting those images can result in a very high loss, 
even the images have no difference in human perception.

<image>

However, with DFC-VAE, the model leverages perceptual loss used in __Neural Style Transfer___.
In ___paper___, it is illustrated that internal representation of convolutional neural networks could capture a content of the input image.
This finding leads to the concept of perceptual loss, which compares the content, hidden representation, between images as oppose to calculate euclidean distant between pixels of two images. 

<image>

## Implementation

The solution contains three file
dfc_vae_model.py
	builds the VAE model, including encoder and decoder
train_dfc_vae.py
	trains the DFC_VAE model
vgg.py
	builds the pre-trained VGG19 model
myutil.py
	contains supporting functions 

To execute, run train_dfc_vae.py

## Dependencies

- train data and test data in tfrecord format



## Reference and Recommended Readings

